{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from model import Fuzzification, InferenceEngine\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.rcParams['font.weight'] = 'bold'\n",
    "plt.rcParams['font.size'] = 12\n",
    "plt.rcParams['axes.labelweight'] = 'bold'\n",
    "plt.rcParams['lines.linewidth'] = 2\n",
    "plt.rcParams['axes.titleweight'] = 'bold'\n",
    "\n",
    "class Fuzzification:\n",
    "    dx = 0.001 # dx for linguistic variable function\n",
    "    nd = 3\n",
    "    #N = 1000\n",
    "    \n",
    "    @staticmethod\n",
    "    def norm_type(iv,typeofnorm,discourse):\n",
    "        \n",
    "        if typeofnorm == 'lower_is_better':\n",
    "            x = Fuzzification.trapezoid(z= discourse,\n",
    "                                        c_l=iv.min(),\n",
    "                                        tc=iv.min(),\n",
    "                                        Tc=iv.min(),\n",
    "                                        c_u=iv.max());\n",
    "    \n",
    "        elif typeofnorm == 'middle_is_better':\n",
    "            x = Fuzzification.trapezoid(z= discourse,\n",
    "                                        c_l=iv.min(),\n",
    "                                        tc=iv.mean(),\n",
    "                                        Tc=iv.mean(),\n",
    "                                        c_u=iv.max());\n",
    "            \n",
    "        elif typeofnorm == 'higher_is_better':\n",
    "            x = Fuzzification.trapezoid(z=discourse,\n",
    "                                        c_l=iv.min(),\n",
    "                                        tc=iv.max(),\n",
    "                                        Tc=iv.max(),\n",
    "                                        c_u=iv.max())\n",
    "        else:\n",
    "            raise 'Error'\n",
    "        return x\n",
    "    \n",
    "    @classmethod\n",
    "    def trapezoid(cls,z,c_l,tc,Tc,c_u):\n",
    "        \"\"\"\n",
    "        Trapezoid function that can be used to create linguistic values or normalization curves.\n",
    "        \n",
    "        Rule #1 of Fuzzy System: Completeness of Inputs - Make sure z covers the full universe of discourse.\n",
    "        \"\"\"\n",
    "#         assert c_l<=tc, 'input values dont make sense for trapezoid'\n",
    "#         assert tc<=Tc, 'input values dont make sense for trapezoid'\n",
    "#         assert Tc<=c_u, 'input values dont make sense for trapezoid'\n",
    "        \n",
    "        if (c_l==tc) and (c_l==Tc): # DECREASING LINE\n",
    "            #print('decreasing line')\n",
    "            out = np.piecewise(z,\n",
    "                               [z<c_l, z>c_u, (z>=Tc)&(z<=c_u)],\n",
    "                               [0    , 0    , lambda z: -z/(c_u-c_l) + c_u/(c_u-c_l)])\n",
    "            \n",
    "        elif (c_u==tc) and (c_u==Tc): # INCREASING LINE\n",
    "            #print('increasing line')\n",
    "            out = np.piecewise(z,\n",
    "                               [z<c_l, z>c_u, (z>=c_l)&(z<=tc)],\n",
    "                               [0    , 0    , lambda z: z/(c_u-c_l) - c_l/(c_u-c_l)])\n",
    "\n",
    "        else: # TRAPEZOID AND TRIANGLE\n",
    "            #print('trapezoid')\n",
    "            out = np.piecewise(z,\n",
    "                               [z<=c_l, z>=c_u, (z>c_l)&(z<tc)          , (z>=tc)&(z<=Tc), (z>Tc)&(z<c_u)],\n",
    "                               [0    , 0    , lambda z: (z-c_l)/(tc-c_l), 1            , lambda z: (c_u-z)/(c_u-Tc)])\n",
    "        out = out.round(cls.nd) #getrid of annoying decimals\n",
    "        return out \n",
    "    \n",
    "    @classmethod\n",
    "    def wms(cls):\n",
    "        \"\"\"\n",
    "        Linguistic Variable WMS\n",
    "        \"\"\"\n",
    "        x= np.arange(0,1+cls.dx,cls.dx).round(cls.nd)\n",
    "\n",
    "        medium_val = 0.7 \n",
    "        df = pd.DataFrame(data = {'w':Fuzzification.trapezoid(x,0,0,0,medium_val),\n",
    "                                  'm':Fuzzification.trapezoid(x,0,medium_val,medium_val,1),\n",
    "                                  's':Fuzzification.trapezoid(x,medium_val,1,1,1)},\n",
    "                          index = x)\n",
    "        Fuzzification.check_ruspini_partition(df)\n",
    "        Fuzzification.check_consistency(df)\n",
    "        return df.round(cls.nd)\n",
    "    \n",
    "    @classmethod\n",
    "    def vbbagvg(cls):\n",
    "        \"\"\"\n",
    "        Linguistic Variable VBBAGVG\n",
    "        \"\"\"\n",
    "        x=np.arange(0,1+cls.dx,cls.dx).round(cls.nd)\n",
    "        d = 0.25\n",
    "        data = {}\n",
    "        for i,l in enumerate(['vb','b','a','g','vg']):\n",
    "            data[l] =Fuzzification.trapezoid(x,d*(i-1),d*(i),d*(i),d*(i+1))\n",
    "        df = pd.DataFrame(data = data,index = x)\n",
    "        cls.check_ruspini_partition(df)\n",
    "        cls.check_consistency(df)\n",
    "        return df.round(cls.nd)\n",
    "    \n",
    "    @classmethod\n",
    "    def elvllflifhhvheh(cls):\n",
    "        \"\"\"\n",
    "        Linguistic Variable elvllflifhhvheh\n",
    "        \"\"\"\n",
    "        x=np.arange(0,1+cls.dx,cls.dx).round(cls.nd)\n",
    "        d = 0.125\n",
    "        data = {}\n",
    "        for i,l in enumerate(['el','vl','l','fl','i','fh','h','vh','eh']):\n",
    "            data[l] =Fuzzification.trapezoid(x,d*(i-1),d*(i),d*(i),d*(i+1))\n",
    "        df = pd.DataFrame(data = data,index = x)\n",
    "        cls.check_ruspini_partition(df)\n",
    "        cls.check_consistency(df)\n",
    "        return df.round(cls.nd)\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_ruspini_partition(df):\n",
    "        \"\"\"\n",
    "        Special case of Rule #2 of Fuzzy System: Consistency of Unions for WMS and VBBAGVG\n",
    "        \"\"\"\n",
    "        assert any(df.sum(axis='columns').apply(lambda x: round(x,2)==1)) == True , 'Ruspini Partition Not Satisfied'\n",
    "    \n",
    "    @staticmethod\n",
    "    def check_consistency(df):\n",
    "        \"\"\"\n",
    "        Rule #2 of Fuzzy System: Consistency of Unions - for any input, the membership functions of all the fuzzy sets it belongs too should be less than or equal to 1.\n",
    "        \"\"\"\n",
    "        assert any(df.sum(axis='columns').apply(lambda x: round(x,2)<=1)) == True , 'Not Consistent'\n",
    "    \n",
    "    @staticmethod\n",
    "    def primary(primary_indicator,basic_indicators,indicator_type):\n",
    "        #construct linguistic value lookuptables\n",
    "        wms = Fuzzification.wms()\n",
    "        #vbbagvg = Fuzzification.vbbagvg()\n",
    "\n",
    "        \n",
    "        #primary indicator db (the right excel tab)\n",
    "        pi_db = pd.read_excel('./databases/indicator_db.xlsx', sheet_name=primary_indicator).round(Fuzzification.nd)\n",
    "    \n",
    "        # CONSTRUCT NORMALIZATION CURVES\n",
    "        curves = []\n",
    "        for basic_indicator in basic_indicators.keys(): \n",
    "            assert pi_db[pi_db[indicator_type]==basic_indicator].shape[1] == 9 , 'Warning, not expected shape' #check,sensitve to number of basic indicators, CHANGE ME\n",
    "            basic_df = pi_db[pi_db[indicator_type]==basic_indicator]\n",
    "            #print(basic_indicator)\n",
    "            iv = basic_df['intensive_value']\n",
    "    \n",
    "            discourse = np.arange(iv.min(),iv.max()+Fuzzification.dx,Fuzzification.dx).round(Fuzzification.nd)\n",
    "            x = Fuzzification.norm_type(iv=iv,\n",
    "                                        typeofnorm=basic_indicators[basic_indicator],\n",
    "                                        discourse=discourse)\n",
    "    \n",
    "            ncurve = pd.Series(data=x.round(Fuzzification.nd),\n",
    "                                index=discourse.round(Fuzzification.nd),\n",
    "                                name=basic_indicator)\n",
    "            curves.append(ncurve)\n",
    "    \n",
    "        # PLOT/SAVE NORMALIZATION CURVES\n",
    "        for curve in curves:\n",
    "            fig = plt.figure(figsize=(10,5));plt.title('Normalization Curve: {}'.format(curve.name));\n",
    "            curve.plot();\n",
    "            plt.grid();plt.xlabel('z [{}]'.format(pi_db[pi_db[indicator_type]==curve.name].iloc[0]['intensive_units']));plt.ylabel('x'); #plt.legend(basic_indicators.keys());\n",
    "            fig.savefig('./outputs/normalization_curves/{}.png'.format(curve.name), dpi=300, bbox_inches='tight')\n",
    "\n",
    "        \n",
    "        #FUZZIFICATION\n",
    "        frames = []\n",
    "        for curve in curves: \n",
    "            df_base = pi_db[pi_db[indicator_type]==curve.name].drop(['raw_value','raw_value_units','intensive_units','source'],axis='columns').reset_index(drop=True) #reset index so that it concats properly\n",
    "            z = df_base['intensive_value'].values\n",
    "            x = curve.loc[z] # pass through normalization curve\n",
    "            wms_v = wms.loc[x.values].reset_index(); #reset index to make concat work\n",
    "            df_out = pd.concat([df_base,wms_v],axis='columns')\n",
    "            assert df_base.shape[0] == df_out.shape[0], 'different amount of rows, error'\n",
    "            frames.append(df_out)\n",
    "        fuzz = pd.concat(frames).reset_index(drop=True)\n",
    "    \n",
    "        #INFERENCE\n",
    "        frames2=[]\n",
    "        for company in fuzz['company'].unique():\n",
    "            frames = []\n",
    "            for year in fuzz['year'].unique():\n",
    "                company_year = fuzz[(fuzz['company'] == company) &\n",
    "                                    (fuzz['year'] == year)]\n",
    "                #print(year,company)\n",
    "                b_indicators = [company_year[company_year[indicator_type]==indicator].iloc[0] for indicator in basic_indicators]\n",
    "                \n",
    "                frames.append(InferenceEngine.b_s(b_indicators)) #apply inference engine\n",
    "            frames2.append(pd.concat(frames,axis='columns').T)\n",
    "        secondary = pd.concat(frames2).reset_index(drop=True)\n",
    "        return secondary\n",
    "\n",
    "\n",
    "\n",
    "class InferenceEngine:\n",
    "    dx = 0.01 # dx for linguistic variable function\n",
    "    nd = 2\n",
    "    \n",
    "    @staticmethod\n",
    "    def osus():\n",
    "        return None\n",
    "    \n",
    "    @staticmethod\n",
    "    def defuzzification():\n",
    "        return None\n",
    "    \n",
    "    #primary indicators\n",
    "\n",
    "    @classmethod\n",
    "    def s_p(cls,secondary_indicators,primary_ind_name,indictor_type):\n",
    "        \"\"\"\n",
    "        SECONDARY TO PRIMARY INFERENCE ENGINE &\n",
    "        PRIMARY TO OSUS\n",
    "    \n",
    "        *** ONLY FOR 2 secondary INDICATOR INPUTS\n",
    "        \"\"\"\n",
    "        #checks\n",
    "        assert all(i['company']==secondary_indicators[0]['company'] for i in secondary_indicators), 'secondary indicators included dont have the same company'\n",
    "        secondary_ind_company = secondary_indicators[0]['company']     \n",
    "    \n",
    "    \n",
    "        assert all(i['year']==secondary_indicators[0]['year'] for i in secondary_indicators), 'secondary indicators included dont have the same year'\n",
    "        secondary_ind_year = secondary_indicators[0]['year']\n",
    "    \n",
    "        #search the rule base for the secondary indicator\n",
    "        rb = pd.read_excel('./databases/rulebase.xlsx', sheet_name=primary_ind_name,skiprows=13)\n",
    "        lval = []\n",
    "        indices = []\n",
    "    \n",
    "        if len(secondary_indicators)==2: #if there are two secondary indicators\n",
    "            for in1_lv in ['vb','b','a','g','vg']:\n",
    "                for in2_lv in ['vb','b','a','g','vg']:\n",
    "                    lval.append(secondary_indicators[0][in1_lv] * secondary_indicators[1][in2_lv]) #LARSEN IMPLICATIONsensitive to the number of indicators\n",
    "                    rule = rb[(rb[secondary_indicators[0][indictor_type]] == in1_lv) &\n",
    "                              (rb[secondary_indicators[1][indictor_type]] == in2_lv)] #selecting the right row from the rulebase, sensitive to the number of indicators\n",
    "                    indices.append(rule[primary_ind_name].iloc[0]) #\n",
    "    \n",
    "        elif len(secondary_indicators)==3: #if there are three secondary indicators\n",
    "            for in1_lv in ['vb','b','a','g','vg']:\n",
    "                for in2_lv in ['vb','b','a','g','vg']:\n",
    "                    for in3_lv in ['vb','b','a','g','vg']:\n",
    "                        lval.append(secondary_indicators[0][in1_lv] * secondary_indicators[1][in2_lv] * secondary_indicators[2][in3_lv]) #LARSEN IMPLICATIONsensitive to the number of indicators\n",
    "                        rule = rb[(rb[secondary_indicators[0][indictor_type]] == in1_lv) &\n",
    "                                  (rb[secondary_indicators[1][indictor_type]] == in2_lv) &\n",
    "                                  (rb[secondary_indicators[2][indictor_type]] == in3_lv)] #selecting the right row from the rulebase, sensitive to the number of indicators\n",
    "                        indices.append(rule[primary_ind_name].iloc[0]) #    \n",
    "        else:\n",
    "            print('WARNING, THER ARE SOME OTHER NUMBER OF secondary INDICATORS')\n",
    "    \n",
    "        s = pd.Series(data=lval,\n",
    "                      index=indices).round(cls.nd)\n",
    "        s = s.groupby(s.index).sum() #add up all similar vals\n",
    "    \n",
    "        s['primary'] = primary_ind_name\n",
    "        s['year'] = secondary_ind_year\n",
    "        s['company'] = secondary_ind_company\n",
    "        return s\n",
    "    \n",
    "    #basic indicators to secondary indicators\n",
    "    @classmethod\n",
    "    def b_s(cls,basic_indicators):\n",
    "        \"\"\"\n",
    "        BASIC TO SECONDARY INFERENCE ENGINE\n",
    "\n",
    "        *** ONLY FOR 2 BASIC INDICATOR INPUTS\n",
    "        \"\"\"\n",
    "        #checks\n",
    "        assert all(i['company']==basic_indicators[0]['company'] for i in basic_indicators), 'basic indicators included dont have the same company'\n",
    "        secondary_ind_company = basic_indicators[0]['company']     \n",
    "\n",
    "        assert all(i['secondary']==basic_indicators[0]['secondary'] for i in basic_indicators), 'basic indicators included dont have the same secondary indicator'\n",
    "        secondary_ind_name = basic_indicators[0]['secondary'] #status\n",
    "\n",
    "        assert all(i['year']==basic_indicators[0]['year'] for i in basic_indicators), 'basic indicators included dont have the same year'\n",
    "        secondary_ind_year = basic_indicators[0]['year']\n",
    "\n",
    "        #search the rule base for the secondary indicator\n",
    "        rb = pd.read_excel('./databases/rulebase.xlsx', sheet_name=secondary_ind_name,skiprows=13)\n",
    "        lval = []\n",
    "        indices = []\n",
    "        \n",
    "        if len(basic_indicators)==2: #if there are two basic indicators\n",
    "            for in1_lv in ['w','m','s']:\n",
    "                for in2_lv in ['w','m','s']:\n",
    "                    lval.append(basic_indicators[0][in1_lv] * basic_indicators[1][in2_lv]) #LARSEN IMPLICATIONsensitive to the number of indicators\n",
    "                    rule = rb[(rb[basic_indicators[0]['basic']] == in1_lv) &\n",
    "                              (rb[basic_indicators[1]['basic']] == in2_lv)] #selecting the right row from the rulebase, sensitive to the number of indicators\n",
    "                    indices.append(rule[secondary_ind_name].iloc[0]) #\n",
    "                    \n",
    "        elif len(basic_indicators)==3: #if there are three basic indicators\n",
    "            for in1_lv in ['w','m','s']:\n",
    "                for in2_lv in ['w','m','s']:\n",
    "                    for in3_lv in ['w','m','s']:\n",
    "                        lval.append(basic_indicators[0][in1_lv] * basic_indicators[1][in2_lv] * basic_indicators[2][in3_lv]) #LARSEN IMPLICATIONsensitive to the number of indicators\n",
    "                        rule = rb[(rb[basic_indicators[0]['basic']] == in1_lv) &\n",
    "                                  (rb[basic_indicators[1]['basic']] == in2_lv) &\n",
    "                                  (rb[basic_indicators[2]['basic']] == in3_lv)] #selecting the right row from the rulebase, sensitive to the number of indicators\n",
    "                        indices.append(rule[secondary_ind_name].iloc[0]) #    \n",
    "        else:\n",
    "            print('WARNING, THER ARE SOME OTHER NUMBER OF BASIC INDICATORS')\n",
    "                \n",
    "        s = pd.Series(data=lval,\n",
    "                      index=indices).round(cls.nd)\n",
    "        s = s.groupby(s.index).sum() #add up all similar vals\n",
    "\n",
    "        s['secondary'] = secondary_ind_name\n",
    "        s['year'] = secondary_ind_year\n",
    "        s['company'] = secondary_ind_company\n",
    "        return s[['secondary','company','year','vb','b','a','g','vg']]\n",
    "\n",
    "    \n",
    "    \n",
    "#################### MAIN.PY ###############\n",
    "\n",
    "struct = {'wealth':{\n",
    "                    'status':{'roe':'middle_is_better',\n",
    "                              'roa':'higher_is_better'},\n",
    "                    'pressure':{'CF_CAPEX':'higher_is_better',\n",
    "                                'dividend payout ratio':'middle_is_better'}, \n",
    "                    'response':{'debt to equity ratio':'middle_is_better',\n",
    "                                'operating expenses':'lower_is_better',\n",
    "                                'effective tax rate':'lower_is_better'}\n",
    "                   },\n",
    "          'ecos':{\n",
    "                    'air':{'clean generation':'lower_is_better', #bc we want to see a low percentage of coal\n",
    "                           'CO2 emissions':'lower_is_better'},\n",
    "                    'land':{'spills':'lower_is_better',\n",
    "                            'solid waste':'lower_is_better',\n",
    "                            'hazardous waste':'lower_is_better'}, \n",
    "                    'water':{'total water withdrawal':'lower_is_better',\n",
    "                             'percent water consumed':'lower_is_better'}\n",
    "                   },\n",
    "          'hums':{\n",
    "                    'health':{'fatalities':'lower_is_better', #bc we want to see a low percentage of coal\n",
    "                              'osha recordable rate (ticr)':'lower_is_better'},\n",
    "                    'polic':{'lobbying spending':'lower_is_better',\n",
    "                            'charitable giving':'higher_is_better'}\n",
    "                   }\n",
    "         }\n",
    "\n",
    "#=============================================================================\n",
    "#=============================================================================\n",
    "print('Starting Basic -> Secondary')\n",
    "\n",
    "frames2 = []\n",
    "for primary_indicator in struct.keys():\n",
    "#     if (primary_indicator == 'wealth') or (primary_indicator == 'ecos'): \n",
    "#         continue # skip for now\n",
    "    frames = []\n",
    "    for secondary_indicator in struct[primary_indicator]:\n",
    "        print('{}-{}'.format(primary_indicator ,secondary_indicator))\n",
    "        frames.append(Fuzzification.primary(primary_indicator=primary_indicator,\n",
    "                                            basic_indicators=struct[primary_indicator][secondary_indicator],\n",
    "                                            indicator_type='basic'))\n",
    "    secondary_agg = pd.concat(frames,axis='index').reset_index(drop=True)\n",
    "    secondary_agg.to_excel('./outputs/{}_secondary_agg.xlsx'.format(primary_indicator))\n",
    "    frames2.append(secondary_agg)\n",
    "\n",
    "#=============================================================================\n",
    "#=============================================================================\n",
    "\n",
    "  \n",
    "\n",
    "#=============================================================================\n",
    "print('Starting Secondary -> Primary')\n",
    "\n",
    "frames3=[]\n",
    "for primary_ind_name in struct.keys():\n",
    "    \n",
    "    print(primary_ind_name)\n",
    "    secondary_agg = pd.read_excel('./outputs/{}_secondary_agg.xlsx'.format(primary_ind_name))\n",
    "    \n",
    "    indicator_type = 'secondary'\n",
    "    #INFERENCE\n",
    "    frames2=[]\n",
    "    for company in secondary_agg['company'].unique():\n",
    "        frames = []\n",
    "        for year in secondary_agg['year'].unique():\n",
    "            company_year = secondary_agg[(secondary_agg['company'] == company) &\n",
    "                                          (secondary_agg['year'] == year)]\n",
    "            #print(year,company)\n",
    "            s_indicators = [company_year[company_year[indicator_type]==indicator].iloc[0] for indicator in secondary_agg[indicator_type].unique()]\n",
    "    \n",
    "            frames.append(InferenceEngine.s_p(s_indicators,primary_ind_name,indicator_type)) #apply inference engine\n",
    "        frames2.append(pd.concat(frames,axis='columns').T)\n",
    "    primary = pd.concat(frames2).reset_index(drop=True)\n",
    "    frames3.append(primary)\n",
    "primary_agg = pd.concat(frames3)\n",
    "\n",
    "primary_agg.to_excel('./outputs/primary_agg.xlsx')\n",
    "#=============================================================================\n",
    "\n",
    "\n",
    "#=============================================================================\n",
    "print('Starting Primary  -> OSUS')\n",
    "\n",
    "primary_agg = pd.read_excel('./outputs/primary_agg.xlsx')\n",
    "\n",
    "osus_ind_name = 'osus'\n",
    "indicator_type = 'primary'\n",
    "#INFERENCE\n",
    "frames2=[]\n",
    "for company in primary_agg['company'].unique():\n",
    "    frames = []\n",
    "    for year in primary_agg['year'].unique():\n",
    "        company_year = primary_agg[(primary_agg['company'] == company) &\n",
    "                                    (primary_agg['year'] == year)]\n",
    "        #print(year,company)\n",
    "        s_indicators = [company_year[company_year[indicator_type]==indicator].iloc[0] for indicator in primary_agg[indicator_type].unique()]\n",
    "\n",
    "        frames.append(InferenceEngine.s_p(s_indicators,osus_ind_name,indicator_type)) #apply inference engine\n",
    "    frames2.append(pd.concat(frames,axis='columns').T)\n",
    "    \n",
    "osus = pd.concat(frames2).reset_index(drop=True)\n",
    "osus.to_excel('./outputs/osus_fuzz.xlsx')\n",
    "#=============================================================================\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "print('Defuzzification')\n",
    "osus = pd.read_excel('./outputs/osus_fuzz.xlsx')\n",
    "osus.index= pd.to_datetime(osus['year'].astype(str), format='%Y')\n",
    "elvllflifhhvheh = Fuzzification.elvllflifhhvheh()\n",
    "\n",
    "\n",
    "osus['crisp']= (elvllflifhhvheh['el'].idxmax()*osus['el'] +\n",
    "                elvllflifhhvheh['vl'].idxmax()*osus['vl'] + \n",
    "                elvllflifhhvheh['l'].idxmax()*osus['l'] + \n",
    "                elvllflifhhvheh['fl'].idxmax()*osus['fl'] + \n",
    "                elvllflifhhvheh['i'].idxmax()*osus['i'] +\n",
    "                elvllflifhhvheh['fh'].idxmax()*osus['fh'] +\n",
    "                elvllflifhhvheh['h'].idxmax()*osus['h'] + \n",
    "                elvllflifhhvheh['vh'].idxmax()*osus['vh'] + \n",
    "                elvllflifhhvheh['eh'].idxmax()*osus['eh'])/osus[['el','vl','l','fl','i','fh','h','vh','eh']].sum(axis='columns')\n",
    "\n",
    "crisp = osus[['company','crisp']]\n",
    "crisp.to_excel('./outputs/osus_crisp.xlsx')\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(8,4))\n",
    "for idx, gp in crisp.groupby('company'):\n",
    "    gp.plot(ax=ax,label=idx)\n",
    "plt.grid(); plt.xlabel('Year'); plt.ylabel('OSUS');plt.legend(crisp.groupby('company').indices)\n",
    "fig.savefig('./outputs/final_output.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "print('COMPLETE')\n",
    "#=============================================================================\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
